{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout\n",
    "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img_rows = 28 \n",
    "img_cols = 28\n",
    "channels = 1\n",
    "img_shape = (img_rows, img_cols, channels)\n",
    "\n",
    "optimizer = Adam(0.0002, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_discriminator():\n",
    "\n",
    "    img_shape = (img_rows, img_cols, channels)\n",
    "    \n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Flatten(input_shape=img_shape))\n",
    "    model.add(Dense(512))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dense(256))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.summary()\n",
    "\n",
    "    img = Input(shape=img_shape)\n",
    "    validity = model(img)\n",
    "\n",
    "    return Model(img, validity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_generator():\n",
    "\n",
    "    noise_shape = (100,)\n",
    "    \n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Dense(256, input_shape=noise_shape))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(Dense(512))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(Dense(1024))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(Dense(np.prod(img_shape), activation='tanh'))\n",
    "    model.add(Reshape(img_shape))\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    noise = Input(shape=noise_shape)\n",
    "    img = model(noise)\n",
    "\n",
    "    return Model(noise, img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_3 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_11 (LeakyReLU)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_12 (LeakyReLU)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 533,505\n",
      "Trainable params: 533,505\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Build and compile the discriminator\n",
    "discriminator = build_discriminator()\n",
    "discriminator.compile(loss='binary_crossentropy', \n",
    "    optimizer=optimizer,\n",
    "    metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_17 (Dense)             (None, 256)               25856     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_13 (LeakyReLU)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_14 (LeakyReLU)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 1024)              525312    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_15 (LeakyReLU)   (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 784)               803600    \n",
      "_________________________________________________________________\n",
      "reshape_2 (Reshape)          (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 1,493,520\n",
      "Trainable params: 1,489,936\n",
      "Non-trainable params: 3,584\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Build and compile the generator\n",
    "generator = build_generator()\n",
    "generator.compile(loss='binary_crossentropy', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The generator takes noise as input and generated imgs\n",
    "z = Input(shape=(100,))\n",
    "img = generator(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For the combined model we will only train the generator\n",
    "discriminator.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The valid takes generated images as input and determines validity\n",
    "valid = discriminator(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The combined model  (stacked generator and discriminator) takes\n",
    "# noise as input => generates images => determines validity \n",
    "combined = Model(z, valid)\n",
    "combined.compile(loss='binary_crossentropy', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_imgs( epoch):\n",
    "    r, c = 5, 5\n",
    "    noise = np.random.normal(0, 1, (r * c, 100))\n",
    "    gen_imgs = generator.predict(noise)\n",
    "\n",
    "    # Rescale images 0 - 1\n",
    "    gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "\n",
    "    fig, axs = plt.subplots(r, c)\n",
    "    cnt = 0\n",
    "    for i in range(r):\n",
    "        for j in range(c):\n",
    "            axs[i,j].imshow(gen_imgs[cnt, :,:,0], cmap='gray')\n",
    "            axs[i,j].axis('off')\n",
    "            cnt += 1\n",
    "    fig.savefig(\"gan/images/mnist_%d.png\" % epoch)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train( epochs, batch_size=128, save_interval=50):\n",
    "\n",
    "    # Load the dataset\n",
    "    (X_train, _), (_, _) = mnist.load_data()\n",
    "\n",
    "    # Rescale -1 to 1\n",
    "    X_train = (X_train.astype(np.float32) - 127.5) / 127.5\n",
    "    X_train = np.expand_dims(X_train, axis=3)\n",
    "\n",
    "    half_batch = int(batch_size / 2)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        # ---------------------\n",
    "        #  Train Discriminator\n",
    "        # ---------------------\n",
    "\n",
    "        # Select a random half batch of images\n",
    "        idx = np.random.randint(0, X_train.shape[0], half_batch)\n",
    "        imgs = X_train[idx]\n",
    "\n",
    "        noise = np.random.normal(0, 1, (half_batch, 100))\n",
    "\n",
    "        # Generate a half batch of new images\n",
    "        gen_imgs = generator.predict(noise)\n",
    "\n",
    "        # Train the discriminator\n",
    "        d_loss_real = discriminator.train_on_batch(imgs, np.ones((half_batch, 1)))\n",
    "        d_loss_fake = discriminator.train_on_batch(gen_imgs, np.zeros((half_batch, 1)))\n",
    "        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "\n",
    "        # ---------------------\n",
    "        #  Train Generator\n",
    "        # ---------------------\n",
    "\n",
    "        noise = np.random.normal(0, 1, (batch_size, 100))\n",
    "\n",
    "        # The generator wants the discriminator to label the generated samples\n",
    "        # as valid (ones)\n",
    "        valid_y = np.array([1] * batch_size)\n",
    "\n",
    "        # Train the generator\n",
    "        g_loss = combined.train_on_batch(noise, valid_y)\n",
    "\n",
    "        # Plot the progress\n",
    "        print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n",
    "\n",
    "        # If at save interval => save generated image samples\n",
    "        if epoch % save_interval == 0:\n",
    "            save_imgs(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eurismar/anaconda3/lib/python3.6/site-packages/keras/engine/training.py:973: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [D loss: 0.204391, acc.: 96.88%] [G loss: 2.054859]\n",
      "1 [D loss: 0.183726, acc.: 90.62%] [G loss: 3.037356]\n",
      "2 [D loss: 0.271733, acc.: 84.38%] [G loss: 3.444073]\n",
      "3 [D loss: 0.467718, acc.: 78.12%] [G loss: 2.234568]\n",
      "4 [D loss: 0.161108, acc.: 100.00%] [G loss: 3.914928]\n",
      "5 [D loss: 1.361989, acc.: 40.62%] [G loss: 1.412842]\n",
      "6 [D loss: 0.221853, acc.: 84.38%] [G loss: 2.814171]\n",
      "7 [D loss: 0.247804, acc.: 90.62%] [G loss: 3.130015]\n",
      "8 [D loss: 0.265296, acc.: 87.50%] [G loss: 3.099335]\n",
      "9 [D loss: 0.205645, acc.: 93.75%] [G loss: 2.574550]\n",
      "10 [D loss: 0.259898, acc.: 84.38%] [G loss: 2.332829]\n",
      "11 [D loss: 0.242731, acc.: 87.50%] [G loss: 3.114260]\n",
      "12 [D loss: 0.394003, acc.: 84.38%] [G loss: 2.329793]\n",
      "13 [D loss: 0.173997, acc.: 96.88%] [G loss: 2.686790]\n",
      "14 [D loss: 0.497098, acc.: 84.38%] [G loss: 2.069629]\n",
      "15 [D loss: 0.200000, acc.: 90.62%] [G loss: 3.376913]\n",
      "16 [D loss: 0.423271, acc.: 78.12%] [G loss: 2.337413]\n",
      "17 [D loss: 0.127643, acc.: 100.00%] [G loss: 2.703805]\n",
      "18 [D loss: 0.288289, acc.: 84.38%] [G loss: 2.798831]\n",
      "19 [D loss: 0.153031, acc.: 96.88%] [G loss: 3.187025]\n",
      "20 [D loss: 0.427233, acc.: 81.25%] [G loss: 2.079052]\n",
      "21 [D loss: 0.267817, acc.: 87.50%] [G loss: 4.135683]\n",
      "22 [D loss: 1.460369, acc.: 40.62%] [G loss: 1.196794]\n",
      "23 [D loss: 0.527019, acc.: 62.50%] [G loss: 2.555158]\n",
      "24 [D loss: 0.133666, acc.: 93.75%] [G loss: 4.082475]\n",
      "25 [D loss: 0.316991, acc.: 87.50%] [G loss: 2.312089]\n",
      "26 [D loss: 0.180544, acc.: 96.88%] [G loss: 3.202943]\n",
      "27 [D loss: 0.376986, acc.: 75.00%] [G loss: 2.269628]\n",
      "28 [D loss: 0.235633, acc.: 96.88%] [G loss: 2.908017]\n",
      "29 [D loss: 0.320910, acc.: 87.50%] [G loss: 3.227175]\n",
      "30 [D loss: 0.471456, acc.: 71.88%] [G loss: 2.891414]\n",
      "31 [D loss: 0.422977, acc.: 81.25%] [G loss: 1.766842]\n",
      "32 [D loss: 0.531287, acc.: 71.88%] [G loss: 2.321935]\n",
      "33 [D loss: 0.391729, acc.: 81.25%] [G loss: 2.670892]\n",
      "34 [D loss: 0.598660, acc.: 65.62%] [G loss: 1.909763]\n",
      "35 [D loss: 0.229484, acc.: 93.75%] [G loss: 3.046244]\n",
      "36 [D loss: 0.778796, acc.: 56.25%] [G loss: 1.572818]\n",
      "37 [D loss: 0.198713, acc.: 96.88%] [G loss: 3.007689]\n",
      "38 [D loss: 1.178864, acc.: 31.25%] [G loss: 1.115989]\n",
      "39 [D loss: 0.347572, acc.: 71.88%] [G loss: 3.113606]\n",
      "40 [D loss: 0.794621, acc.: 50.00%] [G loss: 1.328140]\n",
      "41 [D loss: 0.513800, acc.: 68.75%] [G loss: 2.024181]\n",
      "42 [D loss: 0.324362, acc.: 96.88%] [G loss: 2.212812]\n",
      "43 [D loss: 0.661792, acc.: 62.50%] [G loss: 1.667707]\n",
      "44 [D loss: 0.235047, acc.: 93.75%] [G loss: 2.658327]\n",
      "45 [D loss: 0.551913, acc.: 65.62%] [G loss: 1.496101]\n",
      "46 [D loss: 0.361396, acc.: 81.25%] [G loss: 2.518579]\n",
      "47 [D loss: 0.776906, acc.: 56.25%] [G loss: 1.066089]\n",
      "48 [D loss: 0.529487, acc.: 62.50%] [G loss: 2.629661]\n",
      "49 [D loss: 0.830825, acc.: 46.88%] [G loss: 1.567571]\n",
      "50 [D loss: 0.571949, acc.: 59.38%] [G loss: 2.377645]\n",
      "51 [D loss: 0.922464, acc.: 43.75%] [G loss: 1.278275]\n",
      "52 [D loss: 0.547269, acc.: 68.75%] [G loss: 2.426819]\n",
      "53 [D loss: 0.746846, acc.: 56.25%] [G loss: 1.460747]\n",
      "54 [D loss: 0.594950, acc.: 56.25%] [G loss: 1.758328]\n",
      "55 [D loss: 0.708772, acc.: 59.38%] [G loss: 1.710408]\n",
      "56 [D loss: 0.691963, acc.: 59.38%] [G loss: 1.578210]\n",
      "57 [D loss: 0.609738, acc.: 65.62%] [G loss: 1.902705]\n",
      "58 [D loss: 0.647791, acc.: 59.38%] [G loss: 1.135838]\n",
      "59 [D loss: 0.539738, acc.: 65.62%] [G loss: 1.778569]\n",
      "60 [D loss: 0.956929, acc.: 40.62%] [G loss: 0.999864]\n",
      "61 [D loss: 0.517725, acc.: 71.88%] [G loss: 1.643976]\n",
      "62 [D loss: 0.867932, acc.: 40.62%] [G loss: 1.133652]\n",
      "63 [D loss: 0.595364, acc.: 59.38%] [G loss: 1.505071]\n",
      "64 [D loss: 0.800745, acc.: 46.88%] [G loss: 1.031863]\n",
      "65 [D loss: 0.463364, acc.: 78.12%] [G loss: 1.554990]\n",
      "66 [D loss: 0.701026, acc.: 56.25%] [G loss: 1.130911]\n",
      "67 [D loss: 0.572693, acc.: 62.50%] [G loss: 1.651586]\n",
      "68 [D loss: 1.123487, acc.: 18.75%] [G loss: 0.605387]\n",
      "69 [D loss: 0.609847, acc.: 56.25%] [G loss: 1.381343]\n",
      "70 [D loss: 0.576549, acc.: 65.62%] [G loss: 1.411502]\n",
      "71 [D loss: 0.508420, acc.: 78.12%] [G loss: 1.230774]\n",
      "72 [D loss: 0.811452, acc.: 37.50%] [G loss: 0.821713]\n",
      "73 [D loss: 0.554437, acc.: 68.75%] [G loss: 1.353462]\n",
      "74 [D loss: 0.774528, acc.: 50.00%] [G loss: 1.028682]\n",
      "75 [D loss: 0.725447, acc.: 43.75%] [G loss: 0.904638]\n",
      "76 [D loss: 0.627414, acc.: 62.50%] [G loss: 1.269649]\n",
      "77 [D loss: 1.006087, acc.: 21.88%] [G loss: 0.776120]\n",
      "78 [D loss: 0.672592, acc.: 46.88%] [G loss: 1.070570]\n",
      "79 [D loss: 0.708511, acc.: 50.00%] [G loss: 1.140074]\n",
      "80 [D loss: 0.828081, acc.: 31.25%] [G loss: 0.869365]\n",
      "81 [D loss: 0.720886, acc.: 43.75%] [G loss: 0.881653]\n",
      "82 [D loss: 0.919350, acc.: 37.50%] [G loss: 0.861788]\n",
      "83 [D loss: 0.665519, acc.: 62.50%] [G loss: 0.983847]\n",
      "84 [D loss: 0.857648, acc.: 40.62%] [G loss: 0.811445]\n",
      "85 [D loss: 0.813618, acc.: 43.75%] [G loss: 0.694816]\n",
      "86 [D loss: 0.733183, acc.: 50.00%] [G loss: 0.769399]\n",
      "87 [D loss: 0.646600, acc.: 50.00%] [G loss: 1.020768]\n",
      "88 [D loss: 0.844724, acc.: 31.25%] [G loss: 0.785396]\n",
      "89 [D loss: 0.663804, acc.: 50.00%] [G loss: 0.849909]\n",
      "90 [D loss: 0.709101, acc.: 46.88%] [G loss: 0.909673]\n",
      "91 [D loss: 0.786945, acc.: 37.50%] [G loss: 0.670648]\n",
      "92 [D loss: 0.566224, acc.: 59.38%] [G loss: 1.007949]\n",
      "93 [D loss: 0.728817, acc.: 50.00%] [G loss: 0.827350]\n",
      "94 [D loss: 0.737891, acc.: 37.50%] [G loss: 0.794894]\n",
      "95 [D loss: 0.818728, acc.: 34.38%] [G loss: 0.629311]\n",
      "96 [D loss: 0.698413, acc.: 46.88%] [G loss: 0.700560]\n",
      "97 [D loss: 0.723372, acc.: 46.88%] [G loss: 0.760900]\n",
      "98 [D loss: 0.755255, acc.: 37.50%] [G loss: 0.787366]\n",
      "99 [D loss: 0.673344, acc.: 46.88%] [G loss: 0.790843]\n",
      "100 [D loss: 0.844979, acc.: 21.88%] [G loss: 0.625523]\n"
     ]
    }
   ],
   "source": [
    "train(epochs=101, batch_size=32, save_interval=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
