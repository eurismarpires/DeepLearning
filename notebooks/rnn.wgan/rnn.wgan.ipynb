{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eurismar/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "/home/eurismar/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib.rnn import GRUCell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "flags = tf.app.flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "flags.DEFINE_integer('GEN_STATE_SIZE', 512, '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "flags.DEFINE_float('NOISE_STDEV', 10.0, '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "flags.DEFINE_integer('GEN_GRU_LAYERS', 1, '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FLAGS = flags.FLAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_noise(shape, mean=0.0, stddev=1.0):\n",
    "    return tf.random_normal(shape, mean, stddev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_noise():\n",
    "    noise_shape = [BATCH_SIZE, FLAGS.GEN_STATE_SIZE]\n",
    "    return make_noise(shape=noise_shape, stddev=FLAGS.NOISE_STDEV), noise_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_initial_states(noise):\n",
    "    states = []\n",
    "    for l in range(FLAGS.GEN_GRU_LAYERS):\n",
    "        states.append(noise)\n",
    "    return states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_train_op(cells, char_input, charmap_len, embedding, gt, n_samples, num_neurons, seq_len, sm_bias, sm_weight,\n",
    "                 states):\n",
    "    gt_embedding = tf.reshape(gt, [n_samples * seq_len, charmap_len])\n",
    "    gt_GRU_input = tf.matmul(gt_embedding, embedding)\n",
    "    gt_GRU_input = tf.reshape(gt_GRU_input, [n_samples, seq_len, num_neurons])[:, :-1]\n",
    "    gt_sentence_input = tf.concat([char_input, gt_GRU_input], axis=1)\n",
    "    GRU_output, _ = rnn_step_prediction(cells, charmap_len, gt_sentence_input, num_neurons, seq_len, sm_bias,\n",
    "                                         sm_weight,\n",
    "                                         states)\n",
    "    train_pred = []\n",
    "    # TODO: optimize loop\n",
    "    for i in range(seq_len):\n",
    "        train_pred.append(\n",
    "            tf.concat([tf.zeros([BATCH_SIZE, seq_len - i - 1, charmap_len]), gt[:, :i], GRU_output[:, i:i + 1, :]],\n",
    "                      axis=1))\n",
    "\n",
    "    train_pred = tf.reshape(train_pred, [BATCH_SIZE*seq_len, seq_len, charmap_len])\n",
    "\n",
    "    if FLAGS.LIMIT_BATCH:\n",
    "        indices = tf.random_uniform([BATCH_SIZE], 0, BATCH_SIZE*seq_len, dtype=tf.int32)\n",
    "        train_pred = tf.gather(train_pred, indices)\n",
    "\n",
    "    return train_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_step_prediction(cells, charmap_len, gt_sentence_input, num_neurons, seq_len, sm_bias, sm_weight, states,\n",
    "                        reuse=False):\n",
    "    with tf.variable_scope(\"rnn\", reuse=reuse):\n",
    "        GRU_output = gt_sentence_input\n",
    "        for l in range(FLAGS.GEN_GRU_LAYERS):\n",
    "            GRU_output, states[l] = tf.nn.dynamic_rnn(cells[l], GRU_output, dtype=tf.float32,\n",
    "                                                       initial_state=states[l], scope=\"layer_%d\" % (l + 1))\n",
    "    GRU_output = tf.reshape(GRU_output, [-1, num_neurons])\n",
    "    GRU_output = tf.nn.softmax(tf.matmul(GRU_output, sm_weight) + sm_bias)\n",
    "    GRU_output = tf.reshape(GRU_output, [BATCH_SIZE, -1, charmap_len],name='GRU_output')\n",
    "    return GRU_output, states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_inference_op(cells, char_input, embedding, seq_len, sm_bias, sm_weight, states, num_neurons, charmap_len,\n",
    "                     reuse=False):\n",
    "    inference_pred = []\n",
    "    embedded_pred = [char_input]\n",
    "    for i in range(seq_len):\n",
    "        step_pred, states = rnn_step_prediction(cells, charmap_len, tf.concat(embedded_pred, 1), num_neurons, seq_len,\n",
    "                                                sm_bias, sm_weight, states, reuse=reuse)\n",
    "        best_chars_tensor = tf.argmax(step_pred, axis=2)\n",
    "        best_chars_one_hot_tensor = tf.one_hot(best_chars_tensor, charmap_len)\n",
    "        best_char = best_chars_one_hot_tensor[:, -1, :]\n",
    "        inference_pred.append(tf.expand_dims(best_char, 1))\n",
    "        embedded_pred.append(tf.expand_dims(tf.matmul(best_char, embedding), 1))\n",
    "        reuse = True  # no matter what the reuse was, after the first step we have to reuse the defined vars\n",
    "\n",
    "    return tf.concat(inference_pred, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_train_op(cells, char_input, charmap_len, embedding, gt, n_samples, num_neurons, seq_len, sm_bias, sm_weight,\n",
    "                 states):\n",
    "    gt_embedding = tf.reshape(gt, [n_samples * seq_len, charmap_len])\n",
    "    gt_GRU_input = tf.matmul(gt_embedding, embedding)\n",
    "    gt_GRU_input = tf.reshape(gt_GRU_input, [n_samples, seq_len, num_neurons])[:, :-1]\n",
    "    gt_sentence_input = tf.concat([char_input, gt_GRU_input], axis=1)\n",
    "    GRU_output, _ = rnn_step_prediction(cells, charmap_len, gt_sentence_input, num_neurons, seq_len, sm_bias,\n",
    "                                         sm_weight,\n",
    "                                         states)\n",
    "    train_pred = []\n",
    "    # TODO: optimize loop\n",
    "    for i in range(seq_len):\n",
    "        train_pred.append(\n",
    "            tf.concat([tf.zeros([BATCH_SIZE, seq_len - i - 1, charmap_len]), gt[:, :i], GRU_output[:, i:i + 1, :]],\n",
    "                      axis=1))\n",
    "\n",
    "    train_pred = tf.reshape(train_pred, [BATCH_SIZE*seq_len, seq_len, charmap_len])\n",
    "\n",
    "    if FLAGS.LIMIT_BATCH:\n",
    "        indices = tf.random_uniform([BATCH_SIZE], 0, BATCH_SIZE*seq_len, dtype=tf.int32)\n",
    "        train_pred = tf.gather(train_pred, indices)\n",
    "\n",
    "    return train_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Generator_GRU_CL_VL_TH(n_samples, charmap_len, seq_len=None, gt=None):\n",
    "    with tf.variable_scope(\"Generator\"):\n",
    "        noise, noise_shape = get_noise()\n",
    "        num_neurons = FLAGS.GEN_STATE_SIZE\n",
    "        cells = []\n",
    "        for l in range(FLAGS.GEN_GRU_LAYERS):\n",
    "            cells.append(GRUCell(num_neurons))\n",
    "        # this is separate to decouple train and test\n",
    "        train_initial_states = create_initial_states(noise)\n",
    "        inference_initial_states = create_initial_states(noise)\n",
    "        \n",
    "        sm_weight = tf.Variable(tf.random_uniform([num_neurons, charmap_len], minval=-0.1, maxval=0.1))\n",
    "        sm_bias = tf.Variable(tf.random_uniform([charmap_len], minval=-0.1, maxval=0.1))\n",
    "        \n",
    "        embedding = tf.Variable(tf.random_uniform([charmap_len, num_neurons], minval=-0.1, maxval=0.1))\n",
    "\n",
    "        char_input = tf.Variable(tf.random_uniform([num_neurons], minval=-0.1, maxval=0.1))\n",
    "        char_input = tf.reshape(tf.tile(char_input, [n_samples]), [n_samples, 1, num_neurons])\n",
    "        if seq_len is None:\n",
    "            seq_len = tf.placeholder(tf.int32, None, name=\"ground_truth_sequence_length\")\n",
    "            \n",
    "        if gt is not None: #if no GT, we are training\n",
    "            train_pred = get_train_op(cells, char_input, charmap_len, embedding, gt, n_samples, num_neurons, seq_len,\n",
    "                                      sm_bias, sm_weight, train_initial_states)\n",
    "            inference_op = get_inference_op(cells, char_input, embedding, seq_len, sm_bias, sm_weight, inference_initial_states,\n",
    "                                            num_neurons,\n",
    "                                            charmap_len, reuse=True)\n",
    "        else:\n",
    "            inference_op = get_inference_op(cells, char_input, embedding, seq_len, sm_bias, sm_weight, inference_initial_states,\n",
    "                                            num_neurons,\n",
    "                                            charmap_len, reuse=False)\n",
    "            train_pred = None    \n",
    "            \n",
    "        \n",
    "        return train_pred, inference_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Generator_GRU_CL_VL_TH2(n_samples, charmap_len, seq_len=None, gt=None):\n",
    "    with tf.variable_scope(\"Generator2\"):\n",
    "        noise_shape = [BATCH_SIZE, FLAGS.GEN_STATE_SIZE]\n",
    "        noise = tf.random_normal(shape=noise_shape, mean=0.0, stddev=1.0,name='noise')\n",
    "        num_neurons = FLAGS.GEN_STATE_SIZE\n",
    "        cells = []\n",
    "        for l in range(FLAGS.GEN_GRU_LAYERS):\n",
    "            cells.append(GRUCell(num_neurons))\n",
    "        \n",
    "        states = []\n",
    "        for l in range(FLAGS.GEN_GRU_LAYERS):\n",
    "            states.append(noise)\n",
    "        train_initial_states = states\n",
    "        \n",
    "        states = []\n",
    "        for l in range(FLAGS.GEN_GRU_LAYERS):\n",
    "            states.append(noise)\n",
    "        inference_initial_states = states\n",
    "        \n",
    "        sm_weight = tf.Variable(tf.random_uniform([num_neurons, charmap_len], minval=-0.1, maxval=0.1,name='random_uniform'),name='sm_weitht')\n",
    "        sm_bias = tf.Variable(tf.random_uniform([charmap_len], minval=-0.1, maxval=0.1,name='random_uniform'),name='sm_bias')\n",
    "        \n",
    "        embedding = tf.Variable(tf.random_uniform([charmap_len, num_neurons], minval=-0.1, maxval=0.1,name='random_uniform'),name='embedding')\n",
    "\n",
    "        char_input = tf.Variable(tf.random_uniform([num_neurons], minval=-0.1, maxval=0.1,name='random_uniforme'),name='char_input')\n",
    "        char_input = tf.reshape(tf.tile(char_input, [n_samples]), [n_samples, 1, num_neurons],name='reshape_char_input')\n",
    "        if seq_len is None:\n",
    "            seq_len = tf.placeholder(tf.int32, None, name=\"ground_truth_sequence_length\")\n",
    "            \n",
    "        \n",
    "        if gt is not None:\n",
    "            print('get_train_opt')\n",
    "        else:\n",
    "            inference_pred = []\n",
    "            embedded_pred = [char_input]\n",
    "            for i in range(seq_len):\n",
    "                #step_pred, states = rnn_step_prediction(cells, charmap_len, tf.concat(embedded_pred, 1), num_neurons, seq_len,\n",
    "                #                                sm_bias, sm_weight, states, reuse=reuse)\n",
    "                gt_sentence_input = tf.concat(embedded_pred, 1,name='gt_sentence_input')\n",
    "                with tf.variable_scope(\"rnn\", reuse=False):\n",
    "                    GRU_output = gt_sentence_input\n",
    "                    for l in range(FLAGS.GEN_GRU_LAYERS):\n",
    "                        GRU_output, states[l] = tf.nn.dynamic_rnn(cells[l], GRU_output, dtype=tf.float32,\n",
    "                                                       initial_state=states[l], scope=\"layer_%d\" % (l + 1))\n",
    "                GRU_output = tf.reshape(GRU_output, [-1, num_neurons],name='GRU_output')\n",
    "                GRU_output = tf.nn.softmax(tf.matmul(GRU_output, sm_weight) + sm_bias,name='GRU_softmax')\n",
    "                GRU_output = tf.reshape(GRU_output, [BATCH_SIZE, -1, charmap_len],'reshape_GRU_output')\n",
    "                step_pred = GRU_output\n",
    "                #return GRU_output, states\n",
    "                best_chars_tensor = tf.argmax(step_pred, axis=2,name='best_chars_tensor')\n",
    "                best_chars_one_hot_tensor = tf.one_hot(best_chars_tensor, charmap_len,name='one_hot')\n",
    "                best_char = best_chars_one_hot_tensor[:, -1, :]\n",
    "                inference_pred.append(tf.expand_dims(best_char, 1))\n",
    "                embedded_pred.append(tf.expand_dims(tf.matmul(best_char, embedding), 1,name='expand_dims'))\n",
    "                reuse = True\n",
    "                inference_op = tf.concat(inference_pred, axis=1,name='inference_op')\n",
    "    return train_initial_states,inference_initial_states,noise   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "train_p, infe = Generator_GRU_CL_VL_TH(64,1,1)\n",
    "train_initial_states = Generator_GRU_CL_VL_TH2(64,1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(train_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Generator/concat_1:0\", shape=(64, 1, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(infe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "  train_writer = tf.summary.FileWriter( './logs/1/train ', sess.graph) #tensorboard --logdir logs/1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
