{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from music21 import *\n",
    "from keras.utils import np_utils\n",
    "import pandas as pd\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM\n",
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_fn = 'Allwood_Richard_-_Claro_Pascali_Gaudio.mid'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "midi = converter.parse(data_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "part1 = midi[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "melodia = part1.getElementsByClass(note.Note)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "notas = melodia.getElementsByClass(note.Note)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:3: Music21DeprecationWarning: Note._getMidi was deprecated on May 2014 and will disappear at or after May 2016. use pitch.midi instead\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "raw_text = []\n",
    "for nota in notas:\n",
    "    raw_text.append((str(nota.midi),str(nota.duration.quarterLength)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('65', '1.0'),\n",
       " ('67', '4/3'),\n",
       " ('65', '1/3'),\n",
       " ('65', '1.0'),\n",
       " ('64', '0.75'),\n",
       " ('62', '0.75'),\n",
       " ('62', '4/3'),\n",
       " ('65', '1/3'),\n",
       " ('64', '0.75'),\n",
       " ('65', '1.0')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_text[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create mapping of unique chars to integers\n",
    "chars = sorted(list(set(raw_text))) #pega os caracteres utilizados no texto\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars)) #mapeia cada caracter por um índice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters:  261\n",
      "Total Vocab:  58\n"
     ]
    }
   ],
   "source": [
    "n_chars = len(raw_text) # número total de caracteres no texto\n",
    "n_vocab = len(chars) #numero de caracteres sem repetição\n",
    "print(\"Total Characters: \", n_chars)\n",
    "print(\"Total Vocab: \", n_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Patterns:  251\n"
     ]
    }
   ],
   "source": [
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 10 #tamanho da sequencia\n",
    "dataX = [] \n",
    "dataY = []\n",
    "for i in range(0, n_chars - seq_length, 1): #de zero até o a quantidade caracteres menos o tamanho da sequencia\n",
    "\tseq_in = raw_text[i:i + seq_length]     #pega uma janela do texto\n",
    "\tseq_out = raw_text[i + seq_length]      #uma posição depois da janela\n",
    "\tdataX.append([char_to_int[char] for char in seq_in])\n",
    "\tdataY.append(char_to_int[seq_out])\n",
    "n_patterns = len(dataX)\n",
    "print(\"Total Patterns: \", n_patterns) #qtde de janelas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# reshape X to be [samples, time steps, features]\n",
    "X = numpy.reshape(dataX, (n_patterns, seq_length, 1)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# normalize\n",
    "X = X / float(n_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# one hot encode the output variable\n",
    "y = np_utils.to_categorical(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_5 (LSTM)                (None, 256)               264192    \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 58)                14906     \n",
      "=================================================================\n",
      "Total params: 279,098.0\n",
      "Trainable params: 279,098.0\n",
      "Non-trainable params: 0.0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define the checkpoint\n",
    "filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "208/251 [=======================>......] - ETA: 0s - loss: 3.9732Epoch 00000: loss improved from inf to 3.96606, saving model to weights-improvement-00-3.9661.hdf5\n",
      "251/251 [==============================] - 1s - loss: 3.9661     \n",
      "Epoch 2/200\n",
      "240/251 [===========================>..] - ETA: 0s - loss: 3.6021Epoch 00001: loss improved from 3.96606 to 3.60017, saving model to weights-improvement-01-3.6002.hdf5\n",
      "251/251 [==============================] - 0s - loss: 3.6002     \n",
      "Epoch 3/200\n",
      "240/251 [===========================>..] - ETA: 0s - loss: 3.5583Epoch 00002: loss improved from 3.60017 to 3.57524, saving model to weights-improvement-02-3.5752.hdf5\n",
      "251/251 [==============================] - 0s - loss: 3.5752     \n",
      "Epoch 4/200\n",
      "208/251 [=======================>......] - ETA: 0s - loss: 3.4857Epoch 00003: loss improved from 3.57524 to 3.51655, saving model to weights-improvement-03-3.5165.hdf5\n",
      "251/251 [==============================] - 0s - loss: 3.5165     \n",
      "Epoch 5/200\n",
      "240/251 [===========================>..] - ETA: 0s - loss: 3.5013Epoch 00004: loss did not improve\n",
      "251/251 [==============================] - 0s - loss: 3.5275     \n",
      "Epoch 6/200\n",
      "240/251 [===========================>..] - ETA: 0s - loss: 3.5184Epoch 00005: loss did not improve\n",
      "251/251 [==============================] - 0s - loss: 3.5368     \n",
      "Epoch 7/200\n",
      "224/251 [=========================>....] - ETA: 0s - loss: 3.4853Epoch 00006: loss did not improve\n",
      "251/251 [==============================] - 0s - loss: 3.5254     \n",
      "Epoch 8/200\n",
      "240/251 [===========================>..] - ETA: 0s - loss: 3.5031Epoch 00007: loss improved from 3.51655 to 3.50319, saving model to weights-improvement-07-3.5032.hdf5\n",
      "251/251 [==============================] - 0s - loss: 3.5032     \n",
      "Epoch 9/200\n",
      "224/251 [=========================>....] - ETA: 0s - loss: 3.5225Epoch 00008: loss improved from 3.50319 to 3.49420, saving model to weights-improvement-08-3.4942.hdf5\n",
      "251/251 [==============================] - 0s - loss: 3.4942     \n",
      "Epoch 10/200\n",
      "240/251 [===========================>..] - ETA: 0s - loss: 3.5206Epoch 00009: loss improved from 3.49420 to 3.48992, saving model to weights-improvement-09-3.4899.hdf5\n",
      "251/251 [==============================] - 0s - loss: 3.4899     \n",
      "Epoch 11/200\n",
      "208/251 [=======================>......] - ETA: 0s - loss: 3.5086Epoch 00010: loss improved from 3.48992 to 3.47655, saving model to weights-improvement-10-3.4766.hdf5\n",
      "251/251 [==============================] - 0s - loss: 3.4766     \n",
      "Epoch 12/200\n",
      "208/251 [=======================>......] - ETA: 0s - loss: 3.4441Epoch 00011: loss improved from 3.47655 to 3.44528, saving model to weights-improvement-11-3.4453.hdf5\n",
      "251/251 [==============================] - 0s - loss: 3.4453     \n",
      "Epoch 13/200\n",
      "240/251 [===========================>..] - ETA: 0s - loss: 3.4427Epoch 00012: loss improved from 3.44528 to 3.42684, saving model to weights-improvement-12-3.4268.hdf5\n",
      "251/251 [==============================] - 0s - loss: 3.4268     \n",
      "Epoch 14/200\n",
      "208/251 [=======================>......] - ETA: 0s - loss: 3.3583Epoch 00013: loss improved from 3.42684 to 3.41090, saving model to weights-improvement-13-3.4109.hdf5\n",
      "251/251 [==============================] - 0s - loss: 3.4109     \n",
      "Epoch 15/200\n",
      "224/251 [=========================>....] - ETA: 0s - loss: 3.3296Epoch 00014: loss improved from 3.41090 to 3.33028, saving model to weights-improvement-14-3.3303.hdf5\n",
      "251/251 [==============================] - 0s - loss: 3.3303     \n",
      "Epoch 16/200\n",
      "240/251 [===========================>..] - ETA: 0s - loss: 3.3249Epoch 00015: loss improved from 3.33028 to 3.31501, saving model to weights-improvement-15-3.3150.hdf5\n",
      "251/251 [==============================] - 0s - loss: 3.3150     \n",
      "Epoch 17/200\n",
      "208/251 [=======================>......] - ETA: 0s - loss: 3.3320Epoch 00016: loss did not improve\n",
      "251/251 [==============================] - 0s - loss: 3.3230     \n",
      "Epoch 18/200\n",
      "208/251 [=======================>......] - ETA: 0s - loss: 3.3335Epoch 00017: loss did not improve\n",
      "251/251 [==============================] - 0s - loss: 3.3162     \n",
      "Epoch 19/200\n",
      "208/251 [=======================>......] - ETA: 0s - loss: 3.2982Epoch 00018: loss improved from 3.31501 to 3.28122, saving model to weights-improvement-18-3.2812.hdf5\n",
      "251/251 [==============================] - 0s - loss: 3.2812     \n",
      "Epoch 20/200\n",
      "240/251 [===========================>..] - ETA: 0s - loss: 3.2677Epoch 00019: loss improved from 3.28122 to 3.26515, saving model to weights-improvement-19-3.2652.hdf5\n",
      "251/251 [==============================] - 0s - loss: 3.2652     \n",
      "Epoch 21/200\n",
      "208/251 [=======================>......] - ETA: 0s - loss: 3.2072Epoch 00020: loss improved from 3.26515 to 3.24883, saving model to weights-improvement-20-3.2488.hdf5\n",
      "251/251 [==============================] - 0s - loss: 3.2488     \n",
      "Epoch 22/200\n",
      "208/251 [=======================>......] - ETA: 0s - loss: 3.1677Epoch 00021: loss improved from 3.24883 to 3.19330, saving model to weights-improvement-21-3.1933.hdf5\n",
      "251/251 [==============================] - 0s - loss: 3.1933     \n",
      "Epoch 23/200\n",
      "208/251 [=======================>......] - ETA: 0s - loss: 3.2146Epoch 00022: loss did not improve\n",
      "251/251 [==============================] - 0s - loss: 3.2297     \n",
      "Epoch 24/200\n",
      "208/251 [=======================>......] - ETA: 0s - loss: 3.2645Epoch 00023: loss did not improve\n",
      "251/251 [==============================] - 0s - loss: 3.2522     \n",
      "Epoch 25/200\n",
      "240/251 [===========================>..] - ETA: 0s - loss: 3.2367Epoch 00024: loss did not improve\n",
      "251/251 [==============================] - 0s - loss: 3.2248     \n",
      "Epoch 26/200\n",
      "224/251 [=========================>....] - ETA: 0s - loss: 3.1567Epoch 00025: loss improved from 3.19330 to 3.17962, saving model to weights-improvement-25-3.1796.hdf5\n",
      "251/251 [==============================] - 0s - loss: 3.1796     \n",
      "Epoch 27/200\n",
      "208/251 [=======================>......] - ETA: 0s - loss: 3.2080Epoch 00026: loss improved from 3.17962 to 3.17368, saving model to weights-improvement-26-3.1737.hdf5\n",
      "251/251 [==============================] - 0s - loss: 3.1737     \n",
      "Epoch 28/200\n",
      "208/251 [=======================>......] - ETA: 0s - loss: 3.1576Epoch 00027: loss improved from 3.17368 to 3.17334, saving model to weights-improvement-27-3.1733.hdf5\n",
      "251/251 [==============================] - 0s - loss: 3.1733     \n",
      "Epoch 29/200\n",
      "208/251 [=======================>......] - ETA: 0s - loss: 3.1225Epoch 00028: loss improved from 3.17334 to 3.16025, saving model to weights-improvement-28-3.1602.hdf5\n",
      "251/251 [==============================] - 0s - loss: 3.1602     \n",
      "Epoch 30/200\n",
      "208/251 [=======================>......] - ETA: 0s - loss: 3.1700Epoch 00029: loss improved from 3.16025 to 3.13369, saving model to weights-improvement-29-3.1337.hdf5\n",
      "251/251 [==============================] - 0s - loss: 3.1337     \n",
      "Epoch 31/200\n",
      "240/251 [===========================>..] - ETA: 0s - loss: 3.1488Epoch 00030: loss improved from 3.13369 to 3.12679, saving model to weights-improvement-30-3.1268.hdf5\n",
      "251/251 [==============================] - 0s - loss: 3.1268     \n",
      "Epoch 32/200\n",
      "240/251 [===========================>..] - ETA: 0s - loss: 3.1294Epoch 00031: loss did not improve\n",
      "251/251 [==============================] - 0s - loss: 3.1390     \n",
      "Epoch 33/200\n",
      "208/251 [=======================>......] - ETA: 0s - loss: 3.1190Epoch 00032: loss improved from 3.12679 to 3.11280, saving model to weights-improvement-32-3.1128.hdf5\n",
      "251/251 [==============================] - 0s - loss: 3.1128     \n",
      "Epoch 34/200\n",
      "208/251 [=======================>......] - ETA: 0s - loss: 3.1382Epoch 00033: loss did not improve\n",
      "251/251 [==============================] - 0s - loss: 3.1176     \n",
      "Epoch 35/200\n",
      "208/251 [=======================>......] - ETA: 0s - loss: 3.1226Epoch 00034: loss did not improve\n",
      "251/251 [==============================] - 0s - loss: 3.1435     \n",
      "Epoch 36/200\n",
      "208/251 [=======================>......] - ETA: 0s - loss: 3.0879Epoch 00035: loss improved from 3.11280 to 3.10005, saving model to weights-improvement-35-3.1001.hdf5\n",
      "251/251 [==============================] - 0s - loss: 3.1001     \n",
      "Epoch 37/200\n",
      "240/251 [===========================>..] - ETA: 0s - loss: 3.0388Epoch 00036: loss improved from 3.10005 to 3.03821, saving model to weights-improvement-36-3.0382.hdf5\n",
      "251/251 [==============================] - 0s - loss: 3.0382     \n",
      "Epoch 38/200\n",
      "224/251 [=========================>....] - ETA: 0s - loss: 3.0153Epoch 00037: loss improved from 3.03821 to 3.02154, saving model to weights-improvement-37-3.0215.hdf5\n",
      "251/251 [==============================] - 0s - loss: 3.0215     \n",
      "Epoch 39/200\n",
      "208/251 [=======================>......] - ETA: 0s - loss: 3.0126Epoch 00038: loss improved from 3.02154 to 3.01164, saving model to weights-improvement-38-3.0116.hdf5\n",
      "251/251 [==============================] - 0s - loss: 3.0116     \n",
      "Epoch 40/200\n",
      "208/251 [=======================>......] - ETA: 0s - loss: 3.0235Epoch 00039: loss did not improve\n",
      "251/251 [==============================] - 0s - loss: 3.0357     \n",
      "Epoch 41/200\n",
      "208/251 [=======================>......] - ETA: 0s - loss: 2.9952Epoch 00040: loss improved from 3.01164 to 2.96974, saving model to weights-improvement-40-2.9697.hdf5\n",
      "251/251 [==============================] - 0s - loss: 2.9697     \n",
      "Epoch 42/200\n",
      "224/251 [=========================>....] - ETA: 0s - loss: 2.9594Epoch 00041: loss improved from 2.96974 to 2.94302, saving model to weights-improvement-41-2.9430.hdf5\n",
      "251/251 [==============================] - 0s - loss: 2.9430     \n",
      "Epoch 43/200\n",
      "240/251 [===========================>..] - ETA: 0s - loss: 2.9815Epoch 00042: loss did not improve\n",
      "251/251 [==============================] - 0s - loss: 2.9709     \n",
      "Epoch 44/200\n",
      "208/251 [=======================>......] - ETA: 0s - loss: 2.9394Epoch 00043: loss did not improve\n",
      "251/251 [==============================] - 0s - loss: 2.9680     \n",
      "Epoch 45/200\n",
      "208/251 [=======================>......] - ETA: 0s - loss: 3.0558Epoch 00044: loss did not improve\n",
      "251/251 [==============================] - 0s - loss: 3.0347     \n",
      "Epoch 46/200\n",
      "208/251 [=======================>......] - ETA: 0s - loss: 2.9428Epoch 00045: loss improved from 2.94302 to 2.92111, saving model to weights-improvement-45-2.9211.hdf5\n",
      "251/251 [==============================] - 0s - loss: 2.9211     \n",
      "Epoch 47/200\n",
      "240/251 [===========================>..] - ETA: 0s - loss: 2.8752Epoch 00046: loss improved from 2.92111 to 2.87134, saving model to weights-improvement-46-2.8713.hdf5\n",
      "251/251 [==============================] - 0s - loss: 2.8713     \n",
      "Epoch 48/200\n",
      "240/251 [===========================>..] - ETA: 0s - loss: 2.8238Epoch 00047: loss improved from 2.87134 to 2.84915, saving model to weights-improvement-47-2.8491.hdf5\n",
      "251/251 [==============================] - 0s - loss: 2.8491     \n",
      "Epoch 49/200\n",
      "240/251 [===========================>..] - ETA: 0s - loss: 2.9005Epoch 00048: loss did not improve\n",
      "251/251 [==============================] - 0s - loss: 2.9053     \n",
      "Epoch 50/200\n",
      "224/251 [=========================>....] - ETA: 0s - loss: 2.8159Epoch 00049: loss did not improve\n",
      "251/251 [==============================] - 0s - loss: 2.8766     \n",
      "Epoch 51/200\n",
      "240/251 [===========================>..] - ETA: 0s - loss: 2.8291Epoch 00050: loss improved from 2.84915 to 2.82597, saving model to weights-improvement-50-2.8260.hdf5\n",
      "251/251 [==============================] - 0s - loss: 2.8260     \n",
      "Epoch 52/200\n",
      "224/251 [=========================>....] - ETA: 0s - loss: 2.8693Epoch 00051: loss did not improve\n",
      "251/251 [==============================] - 0s - loss: 2.8760     \n",
      "Epoch 53/200\n",
      "208/251 [=======================>......] - ETA: 0s - loss: 2.8316Epoch 00052: loss improved from 2.82597 to 2.78370, saving model to weights-improvement-52-2.7837.hdf5\n",
      "251/251 [==============================] - 0s - loss: 2.7837     \n",
      "Epoch 54/200\n",
      "208/251 [=======================>......] - ETA: 0s - loss: 2.7684Epoch 00053: loss did not improve\n",
      "251/251 [==============================] - 0s - loss: 2.7909     \n",
      "Epoch 55/200\n",
      "224/251 [=========================>....] - ETA: 0s - loss: 2.7279Epoch 00054: loss improved from 2.78370 to 2.73854, saving model to weights-improvement-54-2.7385.hdf5\n",
      "251/251 [==============================] - 0s - loss: 2.7385     \n",
      "Epoch 56/200\n",
      "224/251 [=========================>....] - ETA: 0s - loss: 2.6662Epoch 00055: loss improved from 2.73854 to 2.66004, saving model to weights-improvement-55-2.6600.hdf5\n",
      "251/251 [==============================] - 0s - loss: 2.6600     \n",
      "Epoch 57/200\n",
      "240/251 [===========================>..] - ETA: 0s - loss: 2.7204Epoch 00056: loss did not improve\n",
      "251/251 [==============================] - 0s - loss: 2.7006     \n",
      "Epoch 58/200\n",
      "208/251 [=======================>......] - ETA: 0s - loss: 2.8051Epoch 00057: loss did not improve\n",
      "251/251 [==============================] - 0s - loss: 2.8313     \n",
      "Epoch 59/200\n",
      "208/251 [=======================>......] - ETA: 0s - loss: 2.7734Epoch 00058: loss did not improve\n",
      "251/251 [==============================] - 0s - loss: 2.7948     \n",
      "Epoch 60/200\n",
      "208/251 [=======================>......] - ETA: 0s - loss: 2.6938Epoch 00059: loss did not improve\n",
      "251/251 [==============================] - 0s - loss: 2.6769     \n",
      "Epoch 61/200\n",
      "208/251 [=======================>......] - ETA: 0s - loss: 2.6123Epoch 00060: loss improved from 2.66004 to 2.64814, saving model to weights-improvement-60-2.6481.hdf5\n",
      "251/251 [==============================] - 0s - loss: 2.6481     \n",
      "Epoch 62/200\n",
      "208/251 [=======================>......] - ETA: 0s - loss: 2.5672Epoch 00061: loss improved from 2.64814 to 2.58730, saving model to weights-improvement-61-2.5873.hdf5\n",
      "251/251 [==============================] - 0s - loss: 2.5873     \n",
      "Epoch 63/200\n",
      "208/251 [=======================>......] - ETA: 0s - loss: 2.5297Epoch 00062: loss did not improve\n",
      "251/251 [==============================] - 0s - loss: 2.5876     \n",
      "Epoch 64/200\n",
      "224/251 [=========================>....] - ETA: 0s - loss: 2.6455Epoch 00063: loss did not improve\n",
      "251/251 [==============================] - 0s - loss: 2.6787     \n",
      "Epoch 65/200\n",
      "208/251 [=======================>......] - ETA: 0s - loss: 2.5563Epoch 00064: loss improved from 2.58730 to 2.55570, saving model to weights-improvement-64-2.5557.hdf5\n",
      "251/251 [==============================] - 0s - loss: 2.5557     \n",
      "Epoch 66/200\n",
      "240/251 [===========================>..] - ETA: 0s - loss: 2.4548Epoch 00065: loss improved from 2.55570 to 2.48019, saving model to weights-improvement-65-2.4802.hdf5\n",
      "251/251 [==============================] - 0s - loss: 2.4802     \n",
      "Epoch 67/200\n",
      "224/251 [=========================>....] - ETA: 0s - loss: 2.5107Epoch 00066: loss did not improve\n",
      "251/251 [==============================] - 0s - loss: 2.5082     \n",
      "Epoch 68/200\n",
      "240/251 [===========================>..] - ETA: 0s - loss: 2.6200Epoch 00067: loss did not improve\n",
      "251/251 [==============================] - 0s - loss: 2.6068     \n",
      "Epoch 69/200\n",
      "240/251 [===========================>..] - ETA: 0s - loss: 2.5156Epoch 00068: loss did not improve\n",
      "251/251 [==============================] - 0s - loss: 2.5242     \n",
      "Epoch 70/200\n",
      "208/251 [=======================>......] - ETA: 0s - loss: 2.5325Epoch 00069: loss did not improve\n",
      "251/251 [==============================] - 0s - loss: 2.5330     \n",
      "Epoch 71/200\n",
      "208/251 [=======================>......] - ETA: 0s - loss: 2.3970Epoch 00070: loss improved from 2.48019 to 2.41625, saving model to weights-improvement-70-2.4163.hdf5\n",
      "251/251 [==============================] - 0s - loss: 2.4163     \n",
      "Epoch 72/200\n",
      "240/251 [===========================>..] - ETA: 0s - loss: 2.4353Epoch 00071: loss did not improve\n",
      "251/251 [==============================] - 0s - loss: 2.4536     \n",
      "Epoch 73/200\n",
      "240/251 [===========================>..] - ETA: 0s - loss: 2.3103Epoch 00072: loss improved from 2.41625 to 2.32391, saving model to weights-improvement-72-2.3239.hdf5\n",
      "251/251 [==============================] - 0s - loss: 2.3239     \n",
      "Epoch 74/200\n",
      "240/251 [===========================>..] - ETA: 0s - loss: 2.3611Epoch 00073: loss did not improve\n",
      "251/251 [==============================] - 0s - loss: 2.3693     \n",
      "Epoch 75/200\n",
      "208/251 [=======================>......] - ETA: 0s - loss: 2.3308Epoch 00074: loss improved from 2.32391 to 2.31244, saving model to weights-improvement-74-2.3124.hdf5\n",
      "251/251 [==============================] - 0s - loss: 2.3124     \n",
      "Epoch 76/200\n",
      "208/251 [=======================>......] - ETA: 0s - loss: 2.2255Epoch 00075: loss improved from 2.31244 to 2.26091, saving model to weights-improvement-75-2.2609.hdf5\n",
      "251/251 [==============================] - 0s - loss: 2.2609     \n",
      "Epoch 77/200\n",
      "208/251 [=======================>......] - ETA: 0s - loss: 2.3304Epoch 00076: loss did not improve\n",
      "251/251 [==============================] - 0s - loss: 2.3671     \n",
      "Epoch 78/200\n",
      "208/251 [=======================>......] - ETA: 0s - loss: 2.3107Epoch 00077: loss did not improve\n",
      "251/251 [==============================] - 0s - loss: 2.3135     \n",
      "Epoch 79/200\n",
      "208/251 [=======================>......] - ETA: 0s - loss: 2.2253Epoch 00078: loss improved from 2.26091 to 2.21985, saving model to weights-improvement-78-2.2198.hdf5\n",
      "251/251 [==============================] - 0s - loss: 2.2198     \n",
      "Epoch 80/200\n",
      "208/251 [=======================>......] - ETA: 0s - loss: 2.1774Epoch 00079: loss improved from 2.21985 to 2.19089, saving model to weights-improvement-79-2.1909.hdf5\n",
      "251/251 [==============================] - 0s - loss: 2.1909     \n",
      "Epoch 81/200\n",
      "208/251 [=======================>......] - ETA: 0s - loss: 2.1792Epoch 00080: loss did not improve\n",
      "251/251 [==============================] - 0s - loss: 2.1992     \n",
      "Epoch 82/200\n",
      "224/251 [=========================>....] - ETA: 0s - loss: 2.1614Epoch 00081: loss improved from 2.19089 to 2.15947, saving model to weights-improvement-81-2.1595.hdf5\n",
      "251/251 [==============================] - 0s - loss: 2.1595     \n",
      "Epoch 83/200\n",
      "208/251 [=======================>......] - ETA: 0s - loss: 2.1361Epoch 00082: loss improved from 2.15947 to 2.11488, saving model to weights-improvement-82-2.1149.hdf5\n",
      "251/251 [==============================] - 0s - loss: 2.1149     \n",
      "Epoch 84/200\n",
      "208/251 [=======================>......] - ETA: 0s - loss: 2.0839Epoch 00083: loss improved from 2.11488 to 2.09648, saving model to weights-improvement-83-2.0965.hdf5\n",
      "251/251 [==============================] - 0s - loss: 2.0965     \n",
      "Epoch 85/200\n",
      "208/251 [=======================>......] - ETA: 0s - loss: 1.9502Epoch 00084: loss improved from 2.09648 to 2.01333, saving model to weights-improvement-84-2.0133.hdf5\n",
      "251/251 [==============================] - 0s - loss: 2.0133     \n",
      "Epoch 86/200\n",
      "208/251 [=======================>......] - ETA: 0s - loss: 2.0519Epoch 00085: loss did not improve\n",
      "251/251 [==============================] - 0s - loss: 2.1043     \n",
      "Epoch 87/200\n",
      "208/251 [=======================>......] - ETA: 0s - loss: 1.9848Epoch 00086: loss did not improve\n",
      "251/251 [==============================] - 0s - loss: 2.0383     \n",
      "Epoch 88/200\n",
      "208/251 [=======================>......] - ETA: 0s - loss: 2.0384Epoch 00087: loss did not improve\n",
      "251/251 [==============================] - 0s - loss: 2.0719     \n",
      "Epoch 89/200\n",
      "208/251 [=======================>......] - ETA: 0s - loss: 2.0090Epoch 00088: loss did not improve\n",
      "251/251 [==============================] - 0s - loss: 2.0239     \n",
      "Epoch 90/200\n",
      "208/251 [=======================>......] - ETA: 0s - loss: 1.9546Epoch 00089: loss improved from 2.01333 to 1.96588, saving model to weights-improvement-89-1.9659.hdf5\n",
      "251/251 [==============================] - 0s - loss: 1.9659     \n",
      "Epoch 91/200\n",
      "208/251 [=======================>......] - ETA: 0s - loss: 1.8933Epoch 00090: loss improved from 1.96588 to 1.90335, saving model to weights-improvement-90-1.9033.hdf5\n",
      "251/251 [==============================] - 0s - loss: 1.9033     \n",
      "Epoch 92/200\n",
      "208/251 [=======================>......] - ETA: 0s - loss: 1.8981Epoch 00091: loss did not improve\n",
      "251/251 [==============================] - 0s - loss: 1.9078     \n",
      "Epoch 93/200\n",
      "208/251 [=======================>......] - ETA: 0s - loss: 1.8616Epoch 00092: loss improved from 1.90335 to 1.88422, saving model to weights-improvement-92-1.8842.hdf5\n",
      "251/251 [==============================] - 0s - loss: 1.8842     \n",
      "Epoch 94/200\n",
      "208/251 [=======================>......] - ETA: 0s - loss: 1.8029Epoch 00093: loss improved from 1.88422 to 1.85592, saving model to weights-improvement-93-1.8559.hdf5\n",
      "251/251 [==============================] - 0s - loss: 1.8559     \n",
      "Epoch 95/200\n",
      "208/251 [=======================>......] - ETA: 0s - loss: 1.7326Epoch 00094: loss improved from 1.85592 to 1.76983, saving model to weights-improvement-94-1.7698.hdf5\n",
      "251/251 [==============================] - 0s - loss: 1.7698     \n",
      "Epoch 96/200\n",
      "208/251 [=======================>......] - ETA: 0s - loss: 1.8577Epoch 00095: loss did not improve\n",
      "251/251 [==============================] - 0s - loss: 1.8684     \n",
      "Epoch 97/200\n",
      "208/251 [=======================>......] - ETA: 0s - loss: 1.7512Epoch 00096: loss improved from 1.76983 to 1.74246, saving model to weights-improvement-96-1.7425.hdf5\n",
      "251/251 [==============================] - 0s - loss: 1.7425     \n",
      "Epoch 98/200\n",
      "208/251 [=======================>......] - ETA: 0s - loss: 1.7871Epoch 00097: loss did not improve\n",
      "251/251 [==============================] - 0s - loss: 1.7696     \n",
      "Epoch 99/200\n",
      "208/251 [=======================>......] - ETA: 0s - loss: 1.7982Epoch 00098: loss did not improve\n",
      "251/251 [==============================] - 0s - loss: 1.8217     \n",
      "Epoch 100/200\n",
      "208/251 [=======================>......] - ETA: 0s - loss: 1.7579Epoch 00099: loss did not improve\n",
      "251/251 [==============================] - 0s - loss: 1.7974     \n",
      "Epoch 101/200\n",
      "208/251 [=======================>......] - ETA: 0s - loss: 1.7471Epoch 00100: loss improved from 1.74246 to 1.72870, saving model to weights-improvement-100-1.7287.hdf5\n",
      "251/251 [==============================] - 0s - loss: 1.7287     \n",
      "Epoch 102/200\n",
      "208/251 [=======================>......] - ETA: 0s - loss: 1.6547Epoch 00101: loss improved from 1.72870 to 1.69089, saving model to weights-improvement-101-1.6909.hdf5\n",
      "251/251 [==============================] - 0s - loss: 1.6909     \n",
      "Epoch 103/200\n",
      "208/251 [=======================>......] - ETA: 0s - loss: 1.6221Epoch 00102: loss did not improve\n",
      "251/251 [==============================] - 0s - loss: 1.7174     \n",
      "Epoch 104/200\n",
      "208/251 [=======================>......] - ETA: 0s - loss: 1.6768Epoch 00103: loss did not improve\n",
      "251/251 [==============================] - 0s - loss: 1.7298     \n",
      "Epoch 105/200\n",
      "208/251 [=======================>......] - ETA: 0s - loss: 1.6537Epoch 00104: loss improved from 1.69089 to 1.65604, saving model to weights-improvement-104-1.6560.hdf5\n",
      "251/251 [==============================] - 0s - loss: 1.6560     \n",
      "Epoch 106/200\n",
      "208/251 [=======================>......] - ETA: 0s - loss: 1.6276Epoch 00105: loss improved from 1.65604 to 1.61138, saving model to weights-improvement-105-1.6114.hdf5\n",
      "251/251 [==============================] - 0s - loss: 1.6114     \n",
      "Epoch 107/200\n",
      "208/251 [=======================>......] - ETA: 0s - loss: 1.5405Epoch 00106: loss improved from 1.61138 to 1.59445, saving model to weights-improvement-106-1.5945.hdf5\n",
      "251/251 [==============================] - 0s - loss: 1.5945     \n",
      "Epoch 108/200\n",
      "208/251 [=======================>......] - ETA: 0s - loss: 1.4794Epoch 00107: loss improved from 1.59445 to 1.49658, saving model to weights-improvement-107-1.4966.hdf5\n",
      "251/251 [==============================] - 0s - loss: 1.4966     \n",
      "Epoch 109/200\n",
      "208/251 [=======================>......] - ETA: 0s - loss: 1.4902Epoch 00108: loss improved from 1.49658 to 1.47052, saving model to weights-improvement-108-1.4705.hdf5\n",
      "251/251 [==============================] - 0s - loss: 1.4705     \n",
      "Epoch 110/200\n",
      "208/251 [=======================>......] - ETA: 0s - loss: 1.3917Epoch 00109: loss improved from 1.47052 to 1.43064, saving model to weights-improvement-109-1.4306.hdf5\n",
      "251/251 [==============================] - 0s - loss: 1.4306     \n",
      "Epoch 111/200\n",
      "208/251 [=======================>......] - ETA: 0s - loss: 1.3904Epoch 00110: loss did not improve\n",
      "251/251 [==============================] - 0s - loss: 1.4357     \n",
      "Epoch 112/200\n",
      "208/251 [=======================>......] - ETA: 0s - loss: 1.4556Epoch 00111: loss did not improve\n",
      "251/251 [==============================] - 0s - loss: 1.5032     \n",
      "Epoch 113/200\n",
      "208/251 [=======================>......] - ETA: 0s - loss: 1.5635Epoch 00112: loss did not improve\n",
      "251/251 [==============================] - 0s - loss: 1.5674     \n",
      "Epoch 114/200\n",
      "208/251 [=======================>......] - ETA: 0s - loss: 1.4590Epoch 00113: loss did not improve\n",
      "251/251 [==============================] - 0s - loss: 1.4714     \n",
      "Epoch 115/200\n",
      "208/251 [=======================>......] - ETA: 0s - loss: 1.5341Epoch 00114: loss did not improve\n",
      "251/251 [==============================] - 0s - loss: 1.5871     \n",
      "Epoch 116/200\n",
      "240/251 [===========================>..] - ETA: 0s - loss: 1.3723Epoch 00115: loss improved from 1.43064 to 1.37939, saving model to weights-improvement-115-1.3794.hdf5\n",
      "251/251 [==============================] - 0s - loss: 1.3794     \n",
      "Epoch 117/200\n",
      "240/251 [===========================>..] - ETA: 0s - loss: 1.3245Epoch 00116: loss improved from 1.37939 to 1.33545, saving model to weights-improvement-116-1.3355.hdf5\n",
      "251/251 [==============================] - 0s - loss: 1.3355     \n",
      "Epoch 118/200\n",
      "240/251 [===========================>..] - ETA: 0s - loss: 1.2769Epoch 00117: loss improved from 1.33545 to 1.29505, saving model to weights-improvement-117-1.2951.hdf5\n",
      "251/251 [==============================] - 0s - loss: 1.2951     \n",
      "Epoch 119/200\n",
      "208/251 [=======================>......] - ETA: 0s - loss: 1.2366Epoch 00118: loss improved from 1.29505 to 1.27836, saving model to weights-improvement-118-1.2784.hdf5\n",
      "251/251 [==============================] - 0s - loss: 1.2784     \n",
      "Epoch 120/200\n",
      "208/251 [=======================>......] - ETA: 0s - loss: 1.2492Epoch 00119: loss improved from 1.27836 to 1.25486, saving model to weights-improvement-119-1.2549.hdf5\n",
      "251/251 [==============================] - 0s - loss: 1.2549     \n",
      "Epoch 121/200\n",
      "208/251 [=======================>......] - ETA: 0s - loss: 1.1991Epoch 00120: loss improved from 1.25486 to 1.22856, saving model to weights-improvement-120-1.2286.hdf5\n",
      "251/251 [==============================] - 0s - loss: 1.2286     \n",
      "Epoch 122/200\n",
      "240/251 [===========================>..] - ETA: 0s - loss: 1.1864Epoch 00121: loss improved from 1.22856 to 1.20692, saving model to weights-improvement-121-1.2069.hdf5\n",
      "251/251 [==============================] - 0s - loss: 1.2069     \n",
      "Epoch 123/200\n",
      "208/251 [=======================>......] - ETA: 0s - loss: 1.3119Epoch 00122: loss did not improve\n",
      "251/251 [==============================] - 0s - loss: 1.3031     \n",
      "Epoch 124/200\n",
      "208/251 [=======================>......] - ETA: 0s - loss: 1.2520Epoch 00123: loss did not improve\n",
      "251/251 [==============================] - 0s - loss: 1.2198     \n",
      "Epoch 125/200\n",
      "224/251 [=========================>....] - ETA: 0s - loss: 1.1349Epoch 00124: loss improved from 1.20692 to 1.14238, saving model to weights-improvement-124-1.1424.hdf5\n",
      "251/251 [==============================] - 0s - loss: 1.1424     \n",
      "Epoch 126/200\n",
      "208/251 [=======================>......] - ETA: 0s - loss: 1.1888Epoch 00125: loss did not improve\n",
      "251/251 [==============================] - 0s - loss: 1.1999     \n",
      "Epoch 127/200\n",
      "208/251 [=======================>......] - ETA: 0s - loss: 1.1562Epoch 00126: loss did not improve\n",
      "251/251 [==============================] - 0s - loss: 1.1529     \n",
      "Epoch 128/200\n",
      "208/251 [=======================>......] - ETA: 0s - loss: 1.1325Epoch 00127: loss improved from 1.14238 to 1.07805, saving model to weights-improvement-127-1.0780.hdf5\n",
      "251/251 [==============================] - 0s - loss: 1.0780     \n",
      "Epoch 129/200\n",
      "208/251 [=======================>......] - ETA: 0s - loss: 1.0735Epoch 00128: loss improved from 1.07805 to 1.04041, saving model to weights-improvement-128-1.0404.hdf5\n",
      "251/251 [==============================] - 0s - loss: 1.0404     \n",
      "Epoch 130/200\n",
      "208/251 [=======================>......] - ETA: 0s - loss: 0.9842Epoch 00129: loss improved from 1.04041 to 1.01605, saving model to weights-improvement-129-1.0160.hdf5\n",
      "251/251 [==============================] - 0s - loss: 1.0160     \n",
      "Epoch 131/200\n",
      "208/251 [=======================>......] - ETA: 0s - loss: 1.0030Epoch 00130: loss improved from 1.01605 to 1.00903, saving model to weights-improvement-130-1.0090.hdf5\n",
      "251/251 [==============================] - 0s - loss: 1.0090     \n",
      "Epoch 132/200\n",
      "208/251 [=======================>......] - ETA: 0s - loss: 1.0451Epoch 00131: loss did not improve\n",
      "251/251 [==============================] - 0s - loss: 1.0626     \n",
      "Epoch 133/200\n",
      "208/251 [=======================>......] - ETA: 0s - loss: 1.1136Epoch 00132: loss did not improve\n",
      "251/251 [==============================] - 0s - loss: 1.1548     \n",
      "Epoch 134/200\n",
      "208/251 [=======================>......] - ETA: 0s - loss: 1.3975Epoch 00133: loss did not improve\n",
      "251/251 [==============================] - 0s - loss: 1.4322     \n",
      "Epoch 135/200\n",
      "208/251 [=======================>......] - ETA: 0s - loss: 1.2727Epoch 00134: loss did not improve\n",
      "251/251 [==============================] - 0s - loss: 1.2725     \n",
      "Epoch 136/200\n",
      "208/251 [=======================>......] - ETA: 0s - loss: 1.0676Epoch 00135: loss did not improve\n",
      "251/251 [==============================] - 0s - loss: 1.1092     \n",
      "Epoch 137/200\n",
      "208/251 [=======================>......] - ETA: 0s - loss: 0.9634Epoch 00136: loss improved from 1.00903 to 0.99365, saving model to weights-improvement-136-0.9936.hdf5\n",
      "251/251 [==============================] - 0s - loss: 0.9936     \n",
      "Epoch 138/200\n",
      "208/251 [=======================>......] - ETA: 0s - loss: 0.9825Epoch 00137: loss improved from 0.99365 to 0.98421, saving model to weights-improvement-137-0.9842.hdf5\n",
      "251/251 [==============================] - 0s - loss: 0.9842     \n",
      "Epoch 139/200\n",
      "208/251 [=======================>......] - ETA: 0s - loss: 1.0171Epoch 00138: loss did not improve\n",
      "251/251 [==============================] - 0s - loss: 1.0921     \n",
      "Epoch 140/200\n",
      "208/251 [=======================>......] - ETA: 0s - loss: 1.0228Epoch 00139: loss did not improve\n",
      "251/251 [==============================] - 0s - loss: 1.0321     \n",
      "Epoch 141/200\n",
      "208/251 [=======================>......] - ETA: 0s - loss: 1.0484Epoch 00140: loss did not improve\n",
      "251/251 [==============================] - 0s - loss: 1.0375     \n",
      "Epoch 142/200\n",
      "208/251 [=======================>......] - ETA: 0s - loss: 0.8640Epoch 00141: loss improved from 0.98421 to 0.89421, saving model to weights-improvement-141-0.8942.hdf5\n",
      "251/251 [==============================] - 0s - loss: 0.8942     \n",
      "Epoch 143/200\n",
      "208/251 [=======================>......] - ETA: 0s - loss: 0.8357Epoch 00142: loss improved from 0.89421 to 0.82894, saving model to weights-improvement-142-0.8289.hdf5\n",
      "251/251 [==============================] - 0s - loss: 0.8289     \n",
      "Epoch 144/200\n",
      "208/251 [=======================>......] - ETA: 0s - loss: 0.8948Epoch 00143: loss did not improve\n",
      "251/251 [==============================] - 0s - loss: 0.8827     \n",
      "Epoch 145/200\n",
      "208/251 [=======================>......] - ETA: 0s - loss: 0.8480Epoch 00144: loss did not improve\n",
      "251/251 [==============================] - 0s - loss: 0.8679     \n",
      "Epoch 146/200\n",
      "208/251 [=======================>......] - ETA: 0s - loss: 0.9732Epoch 00145: loss did not improve\n",
      "251/251 [==============================] - 0s - loss: 0.9935     \n",
      "Epoch 147/200\n",
      "208/251 [=======================>......] - ETA: 0s - loss: 0.9132Epoch 00146: loss did not improve\n",
      "251/251 [==============================] - 0s - loss: 0.9504     \n",
      "Epoch 148/200\n",
      "240/251 [===========================>..] - ETA: 0s - loss: 1.2940Epoch 00147: loss did not improve\n",
      "251/251 [==============================] - 0s - loss: 1.2784     \n",
      "Epoch 149/200\n",
      "240/251 [===========================>..] - ETA: 0s - loss: 0.9192Epoch 00148: loss did not improve\n",
      "251/251 [==============================] - 0s - loss: 0.9046     \n",
      "Epoch 150/200\n",
      "224/251 [=========================>....] - ETA: 0s - loss: 0.8693Epoch 00149: loss did not improve\n",
      "251/251 [==============================] - 0s - loss: 0.8573     \n",
      "Epoch 151/200\n",
      "224/251 [=========================>....] - ETA: 0s - loss: 0.8201Epoch 00150: loss did not improve\n",
      "251/251 [==============================] - 0s - loss: 0.8300     \n",
      "Epoch 152/200\n",
      "224/251 [=========================>....] - ETA: 0s - loss: 0.8911Epoch 00151: loss did not improve\n",
      "251/251 [==============================] - 0s - loss: 0.9186     \n",
      "Epoch 153/200\n",
      "208/251 [=======================>......] - ETA: 0s - loss: 0.7981Epoch 00152: loss improved from 0.82894 to 0.77682, saving model to weights-improvement-152-0.7768.hdf5\n",
      "251/251 [==============================] - 0s - loss: 0.7768     \n",
      "Epoch 154/200\n",
      "208/251 [=======================>......] - ETA: 0s - loss: 0.7081Epoch 00153: loss did not improve\n",
      "251/251 [==============================] - 0s - loss: 0.7955     \n",
      "Epoch 155/200\n",
      "240/251 [===========================>..] - ETA: 0s - loss: 0.8385Epoch 00154: loss did not improve\n",
      "251/251 [==============================] - 0s - loss: 0.8200     \n",
      "Epoch 156/200\n",
      "208/251 [=======================>......] - ETA: 0s - loss: 0.7826Epoch 00155: loss did not improve\n",
      "251/251 [==============================] - 0s - loss: 0.8086     \n",
      "Epoch 157/200\n",
      "208/251 [=======================>......] - ETA: 0s - loss: 0.8548Epoch 00156: loss did not improve\n",
      "251/251 [==============================] - 0s - loss: 0.8893     \n",
      "Epoch 158/200\n",
      "240/251 [===========================>..] - ETA: 0s - loss: 0.8741Epoch 00157: loss did not improve\n",
      "251/251 [==============================] - 0s - loss: 0.8719     \n",
      "Epoch 159/200\n",
      "224/251 [=========================>....] - ETA: 0s - loss: 1.0285Epoch 00158: loss did not improve\n",
      "251/251 [==============================] - 0s - loss: 1.0152     \n",
      "Epoch 160/200\n",
      "208/251 [=======================>......] - ETA: 0s - loss: 0.8105Epoch 00159: loss did not improve\n",
      "251/251 [==============================] - 0s - loss: 0.8384     \n",
      "Epoch 161/200\n",
      "208/251 [=======================>......] - ETA: 0s - loss: 0.8232Epoch 00160: loss did not improve\n",
      "251/251 [==============================] - 0s - loss: 0.8291     \n",
      "Epoch 162/200\n",
      "208/251 [=======================>......] - ETA: 0s - loss: 0.7058Epoch 00161: loss improved from 0.77682 to 0.71100, saving model to weights-improvement-161-0.7110.hdf5\n",
      "251/251 [==============================] - 0s - loss: 0.7110     \n",
      "Epoch 163/200\n",
      "208/251 [=======================>......] - ETA: 0s - loss: 0.6765Epoch 00162: loss improved from 0.71100 to 0.66114, saving model to weights-improvement-162-0.6611.hdf5\n",
      "251/251 [==============================] - 0s - loss: 0.6611     \n",
      "Epoch 164/200\n",
      "208/251 [=======================>......] - ETA: 0s - loss: 0.6362Epoch 00163: loss improved from 0.66114 to 0.63181, saving model to weights-improvement-163-0.6318.hdf5\n",
      "251/251 [==============================] - 0s - loss: 0.6318     \n",
      "Epoch 165/200\n",
      "208/251 [=======================>......] - ETA: 0s - loss: 0.6078Epoch 00164: loss improved from 0.63181 to 0.60728, saving model to weights-improvement-164-0.6073.hdf5\n",
      "251/251 [==============================] - 0s - loss: 0.6073     \n",
      "Epoch 166/200\n",
      "240/251 [===========================>..] - ETA: 0s - loss: 0.5213Epoch 00165: loss improved from 0.60728 to 0.52376, saving model to weights-improvement-165-0.5238.hdf5\n",
      "251/251 [==============================] - 0s - loss: 0.5238     \n",
      "Epoch 167/200\n",
      "208/251 [=======================>......] - ETA: 0s - loss: 0.5288Epoch 00166: loss did not improve\n",
      "251/251 [==============================] - 0s - loss: 0.5287     \n",
      "Epoch 168/200\n",
      "240/251 [===========================>..] - ETA: 0s - loss: 0.5792Epoch 00167: loss did not improve\n",
      "251/251 [==============================] - 0s - loss: 0.5723     \n",
      "Epoch 169/200\n",
      "208/251 [=======================>......] - ETA: 0s - loss: 0.5811Epoch 00168: loss did not improve\n",
      "251/251 [==============================] - 0s - loss: 0.5828     \n",
      "Epoch 170/200\n",
      "208/251 [=======================>......] - ETA: 0s - loss: 0.7204Epoch 00169: loss did not improve\n",
      "251/251 [==============================] - 0s - loss: 0.7881     \n",
      "Epoch 171/200\n",
      "208/251 [=======================>......] - ETA: 0s - loss: 0.6945Epoch 00170: loss did not improve\n",
      "251/251 [==============================] - 0s - loss: 0.7085     \n",
      "Epoch 172/200\n",
      "208/251 [=======================>......] - ETA: 0s - loss: 0.5856Epoch 00171: loss did not improve\n",
      "251/251 [==============================] - 0s - loss: 0.5903     \n",
      "Epoch 173/200\n",
      "240/251 [===========================>..] - ETA: 0s - loss: 0.5945Epoch 00172: loss did not improve\n",
      "251/251 [==============================] - 0s - loss: 0.5964     \n",
      "Epoch 174/200\n",
      "208/251 [=======================>......] - ETA: 0s - loss: 0.5638Epoch 00173: loss did not improve\n",
      "251/251 [==============================] - 0s - loss: 0.5799     \n",
      "Epoch 175/200\n",
      "208/251 [=======================>......] - ETA: 0s - loss: 0.4836Epoch 00174: loss improved from 0.52376 to 0.47542, saving model to weights-improvement-174-0.4754.hdf5\n",
      "251/251 [==============================] - 0s - loss: 0.4754     \n",
      "Epoch 176/200\n",
      "208/251 [=======================>......] - ETA: 0s - loss: 0.5544Epoch 00175: loss did not improve\n",
      "251/251 [==============================] - 0s - loss: 0.5660     \n",
      "Epoch 177/200\n",
      "224/251 [=========================>....] - ETA: 0s - loss: 0.5185Epoch 00176: loss did not improve\n",
      "251/251 [==============================] - 0s - loss: 0.5018     \n",
      "Epoch 178/200\n",
      "208/251 [=======================>......] - ETA: 0s - loss: 0.5640Epoch 00177: loss did not improve\n",
      "251/251 [==============================] - 0s - loss: 0.6560     \n",
      "Epoch 179/200\n",
      "208/251 [=======================>......] - ETA: 0s - loss: 0.5791Epoch 00178: loss did not improve\n",
      "251/251 [==============================] - 0s - loss: 0.5756     \n",
      "Epoch 180/200\n",
      "208/251 [=======================>......] - ETA: 0s - loss: 0.5355Epoch 00179: loss did not improve\n",
      "251/251 [==============================] - 0s - loss: 0.5649     \n",
      "Epoch 181/200\n",
      "240/251 [===========================>..] - ETA: 0s - loss: 0.5853Epoch 00180: loss did not improve\n",
      "251/251 [==============================] - 0s - loss: 0.5909     \n",
      "Epoch 182/200\n",
      "208/251 [=======================>......] - ETA: 0s - loss: 0.5973Epoch 00181: loss did not improve\n",
      "251/251 [==============================] - 0s - loss: 0.6215     \n",
      "Epoch 183/200\n",
      "208/251 [=======================>......] - ETA: 0s - loss: 0.6586Epoch 00182: loss did not improve\n",
      "251/251 [==============================] - 0s - loss: 0.6383     \n",
      "Epoch 184/200\n",
      "208/251 [=======================>......] - ETA: 0s - loss: 0.9841Epoch 00183: loss did not improve\n",
      "251/251 [==============================] - 0s - loss: 1.0048     \n",
      "Epoch 185/200\n",
      "208/251 [=======================>......] - ETA: 0s - loss: 0.8547Epoch 00184: loss did not improve\n",
      "251/251 [==============================] - 0s - loss: 0.8416     \n",
      "Epoch 186/200\n",
      "208/251 [=======================>......] - ETA: 0s - loss: 0.5811Epoch 00185: loss did not improve\n",
      "251/251 [==============================] - 0s - loss: 0.5891     \n",
      "Epoch 187/200\n",
      "224/251 [=========================>....] - ETA: 0s - loss: 0.5450Epoch 00186: loss did not improve\n",
      "251/251 [==============================] - 0s - loss: 0.5512     \n",
      "Epoch 188/200\n",
      "224/251 [=========================>....] - ETA: 0s - loss: 0.4127Epoch 00187: loss improved from 0.47542 to 0.42272, saving model to weights-improvement-187-0.4227.hdf5\n",
      "251/251 [==============================] - 0s - loss: 0.4227     \n",
      "Epoch 189/200\n",
      "240/251 [===========================>..] - ETA: 0s - loss: 0.4137Epoch 00188: loss improved from 0.42272 to 0.41035, saving model to weights-improvement-188-0.4104.hdf5\n",
      "251/251 [==============================] - 0s - loss: 0.4104     \n",
      "Epoch 190/200\n",
      "224/251 [=========================>....] - ETA: 0s - loss: 0.4304Epoch 00189: loss did not improve\n",
      "251/251 [==============================] - 0s - loss: 0.4545     \n",
      "Epoch 191/200\n",
      "208/251 [=======================>......] - ETA: 0s - loss: 0.4573Epoch 00190: loss did not improve\n",
      "251/251 [==============================] - 0s - loss: 0.4551     \n",
      "Epoch 192/200\n",
      "240/251 [===========================>..] - ETA: 0s - loss: 0.4350Epoch 00191: loss did not improve\n",
      "251/251 [==============================] - 0s - loss: 0.4332     \n",
      "Epoch 193/200\n",
      "224/251 [=========================>....] - ETA: 0s - loss: 0.5444Epoch 00192: loss did not improve\n",
      "251/251 [==============================] - 0s - loss: 0.5257     \n",
      "Epoch 194/200\n",
      "224/251 [=========================>....] - ETA: 0s - loss: 0.4652Epoch 00193: loss did not improve\n",
      "251/251 [==============================] - 0s - loss: 0.4610     \n",
      "Epoch 195/200\n",
      "208/251 [=======================>......] - ETA: 0s - loss: 0.3708Epoch 00194: loss did not improve\n",
      "251/251 [==============================] - 0s - loss: 0.4257     \n",
      "Epoch 196/200\n",
      "224/251 [=========================>....] - ETA: 0s - loss: 0.4571Epoch 00195: loss did not improve\n",
      "251/251 [==============================] - 0s - loss: 0.4496     \n",
      "Epoch 197/200\n",
      "240/251 [===========================>..] - ETA: 0s - loss: 0.3929Epoch 00196: loss improved from 0.41035 to 0.40091, saving model to weights-improvement-196-0.4009.hdf5\n",
      "251/251 [==============================] - 0s - loss: 0.4009     \n",
      "Epoch 198/200\n",
      "240/251 [===========================>..] - ETA: 0s - loss: 0.3787Epoch 00197: loss improved from 0.40091 to 0.37948, saving model to weights-improvement-197-0.3795.hdf5\n",
      "251/251 [==============================] - 0s - loss: 0.3795     \n",
      "Epoch 199/200\n",
      "240/251 [===========================>..] - ETA: 0s - loss: 0.3785Epoch 00198: loss improved from 0.37948 to 0.37245, saving model to weights-improvement-198-0.3725.hdf5\n",
      "251/251 [==============================] - 0s - loss: 0.3725     \n",
      "Epoch 200/200\n",
      "208/251 [=======================>......] - ETA: 0s - loss: 0.4156Epoch 00199: loss did not improve\n",
      "251/251 [==============================] - 0s - loss: 0.4059     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a21aa90>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, epochs=200, batch_size=16, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "# load the network weights\n",
    "filename = \"weights-improvement-198-0.3725.hdf5\"\n",
    "model.load_weights(filename)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "int_to_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start = numpy.random.randint(0, len(dataX)-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pattern = dataX[start]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('62', '1.0')\n",
      "('67', '0.75')\n",
      "('65', '0.5')\n",
      "('63', '0.5')\n",
      "('62', '0.5')\n",
      "('60', '0.5')\n",
      "('58', '0.75')\n",
      "('62', '0.75')\n",
      "('65', '4/3')\n",
      "('63', '0.5')\n"
     ]
    }
   ],
   "source": [
    "for value in pattern[:10]:\n",
    "    print(int_to_char[value])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = numpy.reshape(pattern, (1, len(pattern), 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('65', '0.25')\n",
      "('64', '0.25')\n",
      "('64', '0.25')\n",
      "('67', '0.5')\n",
      "('62', '1.0')\n",
      "('62', '1.0')\n",
      "('62', '0.25')\n",
      "('62', '0.25')\n",
      "('60', '0.5')\n",
      "('58', '0.75')\n",
      "\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# generate characters\n",
    "for i in range(10):\n",
    "    x = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "    x = x / float(n_vocab)\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    index = numpy.argmax(prediction)\n",
    "    result = int_to_char[index]\n",
    "    seq_in = [int_to_char[value] for value in pattern]\n",
    "    print(result)\n",
    "    #sys.stdout.write(result)\n",
    "    pattern.append(index)\n",
    "    pattern = pattern[1:len(pattern)]\n",
    "print(\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from fractions import Fraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('64', '0.25') 64 <class 'int'>\n",
      "('67', '0.5') 67 <class 'int'>\n",
      "('69', '0.75') 69 <class 'int'>\n",
      "('67', '0.5') 67 <class 'int'>\n",
      "('63', '0.75') 63 <class 'int'>\n",
      "('69', '0.5') 69 <class 'int'>\n",
      "('64', '0.25') 64 <class 'int'>\n",
      "('65', '0.75') 65 <class 'int'>\n",
      "('69', '0.75') 69 <class 'int'>\n",
      "('65', '0.25') 65 <class 'int'>\n",
      "('62', '0.25') 62 <class 'int'>\n",
      "('62', '0.25') 62 <class 'int'>\n",
      "('67', '0.25') 67 <class 'int'>\n",
      "('65', '0.25') 65 <class 'int'>\n",
      "('64', '0.25') 64 <class 'int'>\n",
      "('60', '0.25') 60 <class 'int'>\n",
      "('69', '0.5') 69 <class 'int'>\n",
      "('62', '0.5') 62 <class 'int'>\n",
      "('62', '1.0') 62 <class 'int'>\n",
      "('62', '4/3') 62 <class 'int'>\n",
      "\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# generate characters\n",
    "stream1 = stream.Stream()\n",
    "for i in range(20):\n",
    "    x = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "    x = x / float(n_vocab)\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    index = numpy.argmax(prediction)\n",
    "    result = int_to_char[index]\n",
    "    seq_in = [int_to_char[value] for value in pattern]    \n",
    "    \n",
    "    #nota.duration.quarterLength = int(result[1])\n",
    "    x = int(result[0])          \n",
    "    y = Fraction(result[1])  \n",
    "    print(result,x,type(x))\n",
    "    n = note.Note()\n",
    "    n.pitch.midi = x \n",
    "    n.duration.quarterLength = y\n",
    "    stream1.append(n)\n",
    "    \n",
    "    #sys.stdout.write(result)\n",
    "    pattern.append(index)\n",
    "    pattern = pattern[1:len(pattern)]   \n",
    "print(\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAuwAAAA5CAYAAABtTH5JAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAc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\nFetF5FPwd2ApzRUf3ygY/weTHH/+DYKGFQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<music21.ipython21.objects.IPythonPNGObject at 0x1df53278>"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stream1.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##gerar uma escala"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAuwAAAA7CAYAAAAghN9CAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAc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\nQmCC\n",
      "text/plain": [
       "<music21.ipython21.objects.IPythonPNGObject at 0x1c0eb320>"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stream2 = stream.Stream()\n",
    "for x in range(60, 81):\n",
    "    n = note.Note()\n",
    "    n.pitch.midi = x\n",
    "    stream2.append(n)\n",
    "stream2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
