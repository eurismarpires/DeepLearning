{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Minimal character-level Vanilla RNN model. Written by Andrej Karpathy (@karpathy)\n",
    "BSD License\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "\n",
    "# data I/O\n",
    "data = open('onu.txt', 'r').read() # should be simple plain text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A embaixadora dos Estados Unidos nas Nações Unidas, Nikki Haley, disse neste domingo (17) que o Conselho de Segurança da Organização das Nações Unidas (ONU) esgotou suas opções na contenção do programa nuclear da Coreia do Norte. Ela ainda afirmou que os EUA podem ter que entregar o assunto ao Pentágono.\\n\"Nós esgotamos todas as coisas que poderíamos fazer no Conselho de Segurança neste momento\", disse Haley ao \"State of the Union\" da CNN, acrescentando que estava perfeitamente feliz em entregar o assunto ao secretário de Defesa, James Mattis.\\n\"Estamos tentando todas as outras possibilidades que temos, mas há muitas opções militares na mesa\", acrescentou.\\n\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "chars = list(set(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_size, vocab_size = len(data), len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 664 characters, 51 unique.\n"
     ]
    }
   ],
   "source": [
    "print('data has %d characters, %d unique.' % (data_size, vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "hidden_size = 100 # size of hidden layer of neurons\n",
    "seq_length = 25 # number of steps to unroll the RNN for\n",
    "learning_rate = 1e-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model parameters\n",
    "Wxh = np.random.randn(hidden_size, vocab_size)*0.01 # input to hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Whh = np.random.randn(hidden_size, hidden_size)*0.01 # hidden to hidden\n",
    "Why = np.random.randn(vocab_size, hidden_size)*0.01 # hidden to output\n",
    "bh = np.zeros((hidden_size, 1)) # hidden bias\n",
    "by = np.zeros((vocab_size, 1)) # output bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lossFun(inputs, targets, hprev):\n",
    "  \"\"\"\n",
    "  inputs,targets are both list of integers.\n",
    "  hprev is Hx1 array of initial hidden state\n",
    "  returns the loss, gradients on model parameters, and last hidden state\n",
    "  \"\"\"\n",
    "  xs, hs, ys, ps = {}, {}, {}, {}\n",
    "  hs[-1] = np.copy(hprev)\n",
    "  loss = 0\n",
    "  # forward pass\n",
    "  for t in range(len(inputs)):\n",
    "    xs[t] = np.zeros((vocab_size,1)) # encode in 1-of-k representation\n",
    "    xs[t][inputs[t]] = 1\n",
    "    hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh) # hidden state\n",
    "    ys[t] = np.dot(Why, hs[t]) + by # unnormalized log probabilities for next chars\n",
    "    ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars\n",
    "    loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss)\n",
    "  # backward pass: compute gradients going backwards\n",
    "  dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "  dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
    "  dhnext = np.zeros_like(hs[0])\n",
    "  for t in reversed(range(len(inputs))):\n",
    "    dy = np.copy(ps[t])\n",
    "    dy[targets[t]] -= 1 # backprop into y. see http://cs231n.github.io/neural-networks-case-study/#grad if confused here\n",
    "    dWhy += np.dot(dy, hs[t].T)\n",
    "    dby += dy\n",
    "    dh = np.dot(Why.T, dy) + dhnext # backprop into h\n",
    "    dhraw = (1 - hs[t] * hs[t]) * dh # backprop through tanh nonlinearity\n",
    "    dbh += dhraw\n",
    "    dWxh += np.dot(dhraw, xs[t].T)\n",
    "    dWhh += np.dot(dhraw, hs[t-1].T)\n",
    "    dhnext = np.dot(Whh.T, dhraw)\n",
    "  for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
    "    np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients\n",
    "  return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample(h, seed_ix, n):\n",
    "  \"\"\" \n",
    "  sample a sequence of integers from the model \n",
    "  h is memory state, seed_ix is seed letter for first time step\n",
    "  \"\"\"\n",
    "  x = np.zeros((vocab_size, 1))\n",
    "  x[seed_ix] = 1\n",
    "  ixes = []\n",
    "  for t in range(n):\n",
    "    h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h) + bh)\n",
    "    y = np.dot(Why, h) + by\n",
    "    p = np.exp(y) / np.sum(np.exp(y))\n",
    "    ix = np.random.choice(range(vocab_size), p=p.ravel())\n",
    "    x = np.zeros((vocab_size, 1))\n",
    "    x[ix] = 1\n",
    "    ixes.append(ix)\n",
    "  return ixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n, p = 0, 0\n",
    "mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "mbh, mby = np.zeros_like(bh), np.zeros_like(by) # memory variables for Adagrad\n",
    "smooth_loss = -np.log(1.0/vocab_size)*seq_length # loss at iteration 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      " oCue eadEósncarNidea\"as ondagoar Nsteqocoas ósnnse Ne Co1uEHes se PrniEfancosbes cnes \"ça onemayeo oto SeinNemeuecriUandahcq lda qdneg, Haa, CUe \n",
      "\"rmaedcs as e\"cidaseeóNaf\"oslegyananedano painesuesonh \n",
      "----\n",
      "iter 200, loss: 96.363967\n",
      "----\n",
      " va oou zeósouas aadogas \"ao õa íomas a1es que rfstasegtas mre sodotos dos ouos des esr\n",
      "dertoonudontogaOg pos pae Oues Unuas qua bes comEotossates Pas Ugitano quásmõesar esogor fonaasgo mes Deugas e ea \n",
      "----\n",
      "iter 300, loss: 93.307151\n",
      "----\n",
      "  ar naiurido augonru Edo da rna \n",
      "ra do Cou Naa arcnimsiehN dotendepEarar orei(al Cuar Enedre Ete aa NOe SoidosCrápme Cqimuas ptuota Crf da nolteganda  an da77õr\"mo. Nq dar E1ridU Ctida Cro do , ddu de \n",
      "----\n",
      "iter 400, loss: 90.065074\n",
      "----\n",
      " Nas q (, das fantáçUas da Unçnlanegnçmas as e (itas lUsinis das disçnidoes, nns ndau pas,ga menNelhaNEnçntçnçdds pos,ades pa seçhes as disa, des qtasdes pedaz qudai7Ur neznque Náa Ha. Js nçõdNNmolto d \n",
      "----\n",
      "iter 500, loss: 86.726326\n",
      "----\n",
      " am lnidixõa as,sa açgoiOo oiiasso aN,, oes bas N,to aõisidoisEdegnís dãomas deliU\", óidos tas Uripçá ahemos po inndos ortárue dos r,is Naçaesoinassmonçaigie anisõo ta, kas isiinõençNas Nnimes bs Unios \n",
      "----\n",
      "iter 600, loss: 83.391416\n",
      "----\n",
      " sass sxaona zosup\"as as petestentapoitós es to. ao Ja es passo ai vo acisso tetdas qu s)uio ses irasihaidas e mieiesgnucmestove pastenestes as que as mostsressssertos aa polas dos queibsta ioidegenrcm \n",
      "----\n",
      "iter 700, loss: 80.033701\n",
      "----\n",
      " e nç Nr Uá Ef , Can a Nte e. felizarynbs dess qua eriza Szliene feiSer nante mertogque tnt no ffas nnreqtese podentr ONmelíz do ao Naçarao,tentamonte.eC) StrepolanEre \", efidnttoleo Un tinie NeAgeEco  \n",
      "----\n",
      "iter 800, loss: 76.694765\n",
      "----\n",
      " nça enaribõe arssuf notontentamorentondasgarertimant. astorentomesíE de tamer ta as Da)te dontoloy quk\"ladas mes oce ne as esdosas as Ue eM bniõorama OD am das fes.ntos mr\n",
      "UA xotanas ráras aonecósunte \n",
      "----\n",
      "iter 900, loss: 73.430528\n",
      "----\n",
      " ougmostos quesarpre or) Halar n) norao.hhorida praaraUgrntalerNOvalaa teidamemiago pa) Upça Cas Noç.rEvugrti(açõertemorates Oeçõenar nodasfes, Uar\n",
      "\"NNOagegpulemaUmocarengotoOPtA eegre NpçEndagartoA ou \n",
      "----\n",
      "iter 1000, loss: 70.166821\n",
      "----\n",
      " siz didie des Naçãe do UNto meiOElanuentameCta. NN, Unsscção tetina  o tra as Unidar rátodes Nuá\"hA por a Codantomo, mes Eoragatça co A es que da nmas Nmçõopamengole) U1nietaões Unoseshe m1ngo aridNlU \n",
      "----\n",
      "iter 1100, loss: 67.012663\n"
     ]
    }
   ],
   "source": [
    "#while True:\n",
    "iter = 0\n",
    "while iter < 1000:\n",
    "  # prepare inputs (we're sweeping from left to right in steps seq_length long)\n",
    "  if p+seq_length+1 >= len(data) or n == 0: \n",
    "    hprev = np.zeros((hidden_size,1)) # reset RNN memory\n",
    "    p = 0 # go from start of data\n",
    "  inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
    "  targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n",
    "\n",
    "  # sample from the model now and then\n",
    "  if n % 100 == 0:\n",
    "    sample_ix = sample(hprev, inputs[0], 200)\n",
    "    txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
    "    print('----\\n %s \\n----' % (txt, ))\n",
    "\n",
    "  # forward seq_length characters through the net and fetch gradient\n",
    "  loss, dWxh, dWhh, dWhy, dbh, dby, hprev = lossFun(inputs, targets, hprev)\n",
    "  smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "  if n % 100 == 0: print('iter %d, loss: %f' % (n, smooth_loss)) # print progress\n",
    "  \n",
    "  # perform parameter update with Adagrad\n",
    "  for param, dparam, mem in zip([Wxh, Whh, Why, bh, by], \n",
    "                                [dWxh, dWhh, dWhy, dbh, dby], \n",
    "                                [mWxh, mWhh, mWhy, mbh, mby]):\n",
    "    mem += dparam * dparam\n",
    "    param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update\n",
    "\n",
    "  p += seq_length # move data pointer\n",
    "  n += 1 # iteration counter \n",
    "  iter = iter + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
